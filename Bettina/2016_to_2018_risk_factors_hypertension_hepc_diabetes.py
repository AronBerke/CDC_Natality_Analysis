# -*- coding: utf-8 -*-
"""2016_to_2018_risk_factors_hypertension_hepC_diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TiQ20HVHuSuPapu_6FjcumvWjrB7bRGm
"""

#import modules
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.model_selection as ms
import pickle
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn import tree
from sklearn import ensemble
import xgboost
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

#from google.colab import drive
#drive.mount('/content/drive')

#Import dataset
all3wolabor_train = pd.read_csv('drive/My Drive/all3wolabor_train.csv')
all3wolabor_test = pd.read_csv(drive/My Drive/all3wolabor_test.csv')

#get rid of 'U' of AB_NICU
all3wolabor_train =  all3wolabor_train[all3wolabor_train['AB_NICU'].isin(['Y', 'N'])]
all3wolabor_test = all3wolabor_test[all3wolabor_test['AB_NICU'].isin(['Y', 'N'])]

print(all3wolabor_train.shape)
print(all3wolabor_test.shape)

all3wolabor_test.columns

def create_random_column(df):
    '''
    this creates a list of random numbers between 1 and 1000
    of the same lenght as each column in the dataframe, appends
    a column named "RANDOM" to the dataframe
    '''
    import random
    mylist = []
    for i in range(0,df.shape[0]):
        x = random.randint(1,1000)
        mylist.append(x)
    df['RANDOM'] = mylist
    
    return df

#cdc3yearswolabor = create_random_column(cdc3yearswolabor)
all3wolabor_train =  create_random_column(all3wolabor_train)
all3wolabor_test = create_random_column(all3wolabor_test)

all3wolabor_test.shape

print(all3wolabor_train['AB_NICU'].unique())
all3wolabor_train['AB_NICU'].value_counts()

all3wolabor_train['RF_PDIAB'].value_counts()

all3wolabor_train['RF_PHYPE'].value_counts()

print(sum(all3wolabor_train['CIG0_R'] >= 5))
print(sum(all3wolabor_train['CIG1_R'] >= 5))

# Create 3 new datasets
#DPLURAL is a very high predictor for NICU! Restrict analysis to single children

# 1. mothers that have risk factor diabetes

p_diabetes_train = all3wolabor_train[all3wolabor_train['RF_PDIAB'] == 'Y']
p_diabetes_test = all3wolabor_test[all3wolabor_test['RF_PDIAB'] == 'Y']
print(p_diabetes_train.shape)

#g_diabetes_train = all3wolabor_train[(all3wolabor_train['RF_GDIAB'] == 'Y') & (all3wolabor_train['DPLURAL'] == 1)]
#g_diabetes_test = all3wolabor_test[(all3wolabor_test['RF_GDIAB'] == 'Y') & (all3wolabor_train['DPLURAL'] == 1)]
#print(g_diabetes_train.shape)

# 2. mothers that have risk factor hypertension
p_hyper_train = all3wolabor_train[all3wolabor_train['RF_PHYPE'] == 'Y']
p_hyper_test = all3wolabor_test[all3wolabor_test['RF_PHYPE'] == 'Y']
print(p_hyper_train.shape)

#g_hyper_train = all3wolabor_train[(all3wolabor_train['RF_GHYPE'] == 'Y') & (all3wolabor_train['DPLURAL'] == 1)]
#g_hyper_test = all3wolabor_test[(all3wolabor_test['RF_GHYPE'] == 'Y') & (all3wolabor_train['DPLURAL'] == 1)]
#print(g_hyper_train.shape)

# 3. mothers that have risk factor hypertension
hepC_train = all3wolabor_train[(all3wolabor_train['IP_HEPC'] == 'Y') & (all3wolabor_train['DPLURAL'] == 1)]
hepC_test = all3wolabor_test[(all3wolabor_test['IP_HEPC'] == 'Y') & (all3wolabor_train['DPLURAL'] == 1)]
print(hepC_train.shape)

# 4. mothers that smoked more than 40 cigarettes before pregnancy and the first trimester
#smoker_train = all3wolabor_train[all3wolabor_train['CIG1_R'] == 5]
#smoker_test = all3wolabor_test[all3wolabor_test['CIG1_R'] == 5]
#print(smoker_train.shape)

#LabelEncoding Function. Thanks Ira!
def LabelEncoding(dataframe):
    '''
    Function that takes a dataframe and transforms it with label encoding on all the categorical features.
    '''
    
    #create a list using object types since dataframe.dtypes.value_counts() only shows objects and int64
    objlist = list(dataframe.select_dtypes(include=['object']).columns)
    
    #change type then transform column using cat codes
    for col in objlist:
        dataframe[col] = dataframe[col].astype('category')
        dataframe[col] = dataframe[col].cat.codes
    
    return dataframe

# Label encode test and train

g_diabetes_train = LabelEncoding(g_diabetes_train)
g_diabetes_test = LabelEncoding(g_diabetes_test)

g_hyper_train = LabelEncoding(g_hyper_train)
g_hyper_test = LabelEncoding(g_hyper_test)

hepC_train = LabelEncoding(hepC_train)
hepC_test = LabelEncoding(hepC_test)

print(g_hyper_train['AB_NICU'].value_counts())
14722*100/(61014+14722)

print(hepC_train['AB_NICU'].value_counts())
1160*100/(3960+1160)

cor = g_hyper_train.corr()
plt.figure(figsize=(12,8))
sns.heatmap(cor,annot=True)
plt.tight_layout()

X_train_dia = train_dia.drop('AB_NICU', axis=1)
y_train_dia = train_dia['AB_NICU']
X_test_dia = test_dia.drop('AB_NICU', axis=1)
y_test_dia = test_dia['AB_NICU']

#X_train_g_dia = g_diabetes_train.drop('AB_NICU', axis=1)
#y_train_g_dia = g_diabetes_train['AB_NICU']
#X_test_g_dia = g_diabetes_test.drop('AB_NICU', axis=1)
#y_test_g_dia = g_diabetes_test['AB_NICU']
#y_train_g_dia.value_counts()

X_train_hyper = train_hyper.drop('AB_NICU', axis=1)
y_train_hyper = train_hyper['AB_NICU']
X_test_hyper = test_hyper.drop('AB_NICU', axis=1)
y_test_hyper = test_hyper['AB_NICU']
y_train_hyper.value_counts()

#X_train_g_hyper = g_hyper_train.drop('AB_NICU', axis=1)
#y_train_g_hyper = g_hyper_train['AB_NICU']
#X_test_g_hyper = g_hyper_test.drop('AB_NICU', axis=1)
#y_test_g_hyper = g_hyper_test['AB_NICU']
#y_train_g_hyper.value_counts()

#X_train_smoker = train_smoker.drop('AB_NICU', axis=1)
#y_train_smoker = train_smoker['AB_NICU']
#X_test_smoker = test_smoker.drop('AB_NICU', axis=1)
#y_test_smoker = test_smoker['AB_NICU']
#y_train_smoker.value_counts()

X_train_hepC = hepC_train.drop('AB_NICU', axis=1)
X_train_min_hepC = hepC_train.drop(['CA_CCHD', 'CA_OMPH', 'CA_LIMB', 'CA_CLPAL', 'CA_DISOR', 'DMAR', 'MAR_P', 'AB_NICU'], axis=1)
y_train_hepC = hepC_train['AB_NICU']

X_test_hepC = hepC_test.drop('AB_NICU', axis=1)
X_test_min_hepC = hepC_train.drop(['CA_CCHD', 'CA_OMPH', 'CA_LIMB', 'CA_CLPAL', 'CA_DISOR', 'DMAR', 'MAR_P','AB_NICU'], axis=1)
y_test_hepC = hepC_test['AB_NICU']
y_train_hepC.value_counts()

print(y_train_dia.value_counts())
print(y_train_hyper.value_counts())
print(y_train_hepC.value_counts())

# HEPC XGBoost initial fit - reduced features #########
xgb = XGBClassifier()
xgb.set_params(random_state=0)
xgb.fit(X_train_hepC, y_hepC)
print("The training error is: %.5f" % (1 - xgb.score(X_train_hepC, y_train_hepC)))
print("The test error is: %.5f" % (1 - xgb.score(X_test_hepC, y_test_hepC)))

# here 3:1 ratio non-NICU vs NICU 
xgb1 = XGBClassifier()
xgb1.set_params(random_state=0, scale_pos_weight = 3)
xgb1.fit(X_train_min_hepC, y_train_hepC)
print("The training error is: %.5f" % (1 - xgb1.score(X_train_min_hepC, y_train_hepC)))
print("The test error is: %.5f" % (1 - xgb1.score(X_test_min_hepC, y_test_hepC)))

cm_hepC = confusion_matrix(y_test_hepC, xgb.predict(X_test_hepC))
print(cm_hepC)

cm_hepC_weight = confusion_matrix(y_test_hepC, xgb1.predict(X_test_hepC))
print(cm_hepC_weight)

# HEPC XGBoost initial fit #########
xgb = XGBClassifier()
xgb.set_params(random_state=0)
xgb.fit(X_train_hepC, y_train_hepC)
print("The training error is: %.5f" % (1 - xgb.score(X_train_hepC, y_train_hepC)))
print("The test error is: %.5f" % (1 - xgb.score(X_test_hepC, y_test_hepC)))

# here balanced set, so this misclassifies! 
xgb1 = XGBClassifier()
xgb1.set_params(random_state=0, scale_pos_weight = 3)
xgb1.fit(X_train_hepC, y_train_hepC)
print("The training error is: %.5f" % (1 - xgb1.score(X_train_hepC, y_train_hepC)))
print("The test error is: %.5f" % (1 - xgb1.score(X_test_hepC, y_test_hepC)))

# Commented out IPython magic to ensure Python compatibility.
# set the parameter grid HepC
xgb_param_grid ={'learning_rate': [0.01, 0.05, 0.1],
                 'max_depth': [7,8,9],
                 'min_child_weight': [6,7,8],
                 'n_estimators': [800, 900, 1000, 1100]}

grid_search_xgb_hepC_prec = GridSearchCV(xgb, xgb_param_grid, scoring='precision', cv= 3, n_jobs=-1, return_train_score = True)
# %time grid_search_xgb_hepC_prec.fit(X_train_hepC, y_train_hepC)

# get the best parameters HepC
print(grid_search_xgb_hepC_prec.best_params_)
print(grid_search_xgb_hepC_prec.best_score_)

confusion_matrix(y_test_hepC, grid_search_xgb_hepC_prec.best_estimator_.predict(X_test_hepC))

# Commented out IPython magic to ensure Python compatibility.
# set the parameter grid HepC
xgb_param_grid ={'learning_rate': [0.001, 0.01, 0.05],
                 'max_depth': [8, 9, 10],
                 'min_child_weight': [3,4,5],
                 'n_estimators': [600, 700, 800]}

grid_search_xgb_hepC_recall = GridSearchCV(xgb, xgb_param_grid, scoring='balanced_accuracy', cv= 3, n_jobs=-1, return_train_score = True)
# %time grid_search_xgb_hepC_recall.fit(X_train_hepC, y_train_hepC)

# get the best parameters HepC
print(grid_search_xgb_hepC_recall.best_params_)
print(grid_search_xgb_hepC_recall.best_score_)

# confusion matrix gridsearch HepC
confusion_matrix(y_test_hepC, grid_search_xgb_hepC_recall.best_estimator_.predict(X_test_hepC))

# Get numerical feature importances
importances_xgb = list(xgb.feature_importances_)
# List of tuples with variable and importance
feature_importances_xgb = [(feature, round(importance, 5)) for feature, importance in zip(X_train_hepC.columns, importances_xgb)]
# Sort the feature importances by most important first
xgb_feature_importances = sorted(feature_importances_xgb, key = lambda x: x[1], reverse = True )
# Print out the feature and importances 
[print('Variable: {:10} Importance: {}'.format(*pair)) for pair in xgb_feature_importances]

# Get numerical feature importances
importances_grid_xgb_hyper = list(grid_search_xgb_hepC_prec.best_estimator_.feature_importances_)
# List of tuples with variable and importance
feature_importances_grid_xgb_hyper = [(feature, round(importance, 5)) for feature, importance in zip(X_train_hyper.columns, importances_grid_xgb_hyper)]
# Sort the feature importances by most important first
xgb_grid_feature_importances = sorted(feature_importances_grid_xgb_hyper, key = lambda x: x[1], reverse = True )
# Print out the feature and importances 
[print('Variable: {:10} Importance: {}'.format(*pair)) for pair in xgb_grid_feature_importances]

#XGBoost initial fit hypertension
xgbh = XGBClassifier()
xgbh.set_params(random_state=0)
xgbh.fit(X_train_hyper, y_train_hyper)
print("The training error is: %.5f" % (1 - xgbh.score(X_train_hyper, y_train_hyper)))
print("The test error is: %.5f" % (1 - xgbh.score(X_train_hyper, y_train_hyper)))

#XGBoost initial fit hypertension weighted
xgbh1 = XGBClassifier()
xgbh1.set_params(random_state=0, scale_pos_weight = 5)
xgbh1.fit(X_train_hyper, y_train_hyper)
print("The training error is: %.5f" % (1 - xgbh1.score(X_train_hyper, y_train_hyper)))
print("The test error is: %.5f" % (1 - xgbh1.score(X_train_hyper, y_train_hyper)))

cm_test = confusion_matrix(y_test_hyper, xgbh.predict(X_test_hyper))
print(cm_test)
cm_test2 = confusion_matrix(y_test_hyper, xgbh1.predict(X_test_hyper))
print(cm_test2)

# Commented out IPython magic to ensure Python compatibility.
# set the parameter grid Hypertension
xgb_param_grid ={'learning_rate': [0.01, 0.05, 0.1],
                 'max_depth': [7,8,9],
                 'min_child_weight': [5,6,7],
                 'n_estimators': [600, 700, 800]}

grid_search_xgb_hyper = GridSearchCV(xgbh1, xgb_param_grid, scoring='precision', cv= 3, n_jobs=-1, return_train_score = True)
# %time grid_search_xgb_hyper.fit(X_train_hyper, y_train_hyper)

# get the best parameters Hypertension
print(grid_search_xgb_hyper.best_params_)
print(grid_search_xgb_hyper.best_score_)

# confusion matrix gridsearch Hypertension
confusion_matrix(y_test_hyper, grid_search_xgb_hyper.best_estimator_.predict(X_test_hyper))

# Commented out IPython magic to ensure Python compatibility.
#grid search
grid_search_xgb_hyper1 = GridSearchCV(xgbh1, xgb_param_grid, scoring='roc_auc', cv= 5, n_jobs=-1, return_train_score = True)
# %time grid_search_hyper1.fit(X_train_hyper, y_train_hyper)

# get the best parameters
print(grid_search_xgb_hyper1.best_params_)
print(grid_search_xgb_hyper1.best_score_)
# confusion matrix
confusion_matrix(y_test_hyper, grid_search_xgb_hyper1.best_estimator_.predict(X_test_hyper))

# Commented out IPython magic to ensure Python compatibility.
#grid search
grid_search_xgb_hyper2 = GridSearchCV(xgbh1, xgb_param_grid, scoring='recall', cv= 5, n_jobs=-1, return_train_score = True)
# %time grid_search_xgb_hyper2.fit(X_train_hyper, y_train_hyper)

# get the best parameters
print(grid_search_xgb_hyper2.best_params_)
print(grid_search_xgb_hyper2.best_score_)
# confusion matrix
confusion_matrix(y_test_hyper, grid_search_xgb_hyper2.best_estimator_.predict(X_test_hyper))

# Commented out IPython magic to ensure Python compatibility.
grid_search_xgb_hyper3 = GridSearchCV(xgbh1, xgb_param_grid, scoring='f1', cv= 5, n_jobs=-1, return_train_score = True)
# %time grid_search_xgb_hyper3.fit(X_train_hyper, y_train_hyper)

# get the best parameters
print(grid_search_xgb_hyper3.best_params_)
print(grid_search_xgb_hyper3.best_score_)
# confusion matrix
confusion_matrix(y_test_hyper, grid_search_xgb_hyper3.best_estimator_.predict(X_test_hyper))

# Get numerical feature importances
importances_xgb_hyper = list(xgbh1.feature_importances_)
# List of tuples with variable and importance
feature_importances_xgb_hyper = [(feature, round(importance, 5)) for feature, importance in zip(X_train_hyper.columns, importances_xgb_hyper)]
# Sort the feature importances by most important first
xgb_feature_importances = sorted(feature_importances_xgb_hyper, key = lambda x: x[1], reverse = True )
# Print out the feature and importances 
[print('Variable: {:10} Importance: {}'.format(*pair)) for pair in xgb_feature_importances]

# change cut of for probabilities 
test_hyper_predprob = grid_search_xgb.predict_proba(X_test_hyper)[:,1] # should be the 1st
test_hyper_predprob[test_hyper_predprob>=0.4]=1 
test_hyper_predprob[test_hyper_predprob<0.4]=0

# create new confusion matrix
confusion_matrix(y_test_dia, test_dia_predprob)

#Prediction with tuned hyperparameters
grid_xgb_pred = grid_search_xgb.predict(X_test_hyper)
grid_xgb_pred

# Get numerical feature importances
importances_xgb = list(xgb.feature_importances_)

# List of tuples with variable and importance
feature_importances_xgb = [(feature, round(importance, 5)) for feature, importance in zip(X_train.columns, importances_xgb)]

# Sort the feature importances by most important first
xgb_feature_importances = sorted(feature_importances_xgb, key = lambda x: x[1], reverse = True )

# Print out the feature and importances 
[print('Variable: {:10} Importance: {}'.format(*pair)) for pair in xgb_feature_importances]

xgb_feature_importances_top20 = xgb_feature_importances[:20]
featureNames, featureScores = zip(*list(xgb_feature_importances_top20))
xgb_feature_importances_top20

plt.barh(range(len(featureScores)), featureScores, tick_label=featureNames)
plt.gca().invert_yaxis()
plt.title('feature importance')
plt.ylabel('Features')
plt.xlabel('Importance Score')
plt.title('Feature Importances')
plt.savefig('xgbFI.png')

# Commented out IPython magic to ensure Python compatibility.
# set the parameter grid
xgb_param_grid_3 ={'learning_rate': [0.05],
                 'max_depth': [5],
                 'min_child_weight': [4],
                 'n_estimators': [300]}

scoring = {'acc': 'accuracy',
           'prec': 'precision',
           'f1': 'f1',
           're':'recall',
           'auc': 'roc_auc'
          }

#grid search
grid_search_xgb_3 = GridSearchCV(xgb_1, xgb_param_grid_3, scoring=scoring, cv= 3, n_jobs=-1, return_train_score = True, refit=False)
# %time grid_search_xgb_3.fit(X_train, y_train)

grid_search_xgb_3.cv_results_

# set the parameter grid
#xgb_param_grid ={'learning_rate': [0.001, 0.01, 0.05],
#                 'max_depth': [4,5,6],
#                 'min_child_weight': [4,5,6],
#                 'n_estimators': [200, 300, 400, 500]}

#from sklearn.model_selection import StratifiedKFold
#kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)


#grid search
#grid_search_xgb_3 = GridSearchCV(xgb_2, xgb_param_grid, scoring='precision', cv= kfold, n_jobs=-1, return_train_score = True)
#%time grid_search_xgb_3.fit(X_train_hyper, y_train_hyper)

import pickle
# save model to file
#grid_search_xgb.save_model('downsampled_2016_accuracy.model')
 pickle.dump(xgb_1, open("2016beforelabor_downsampled.pickle.dat", "wb"))
# load model later
# loaded_model = pickle.load(open("2016beforelabor_downsampled.pickle.dat", "rb"))
# make predictions for test data
#predictions = loaded_model.predict(X_test)
# evaluate predictions
#accuracy = accuracy_score(y_test, predictions)
#print("Accuracy: %.2f%%" % (accuracy * 100.0))

# hypertension_df
# reduce feature space 
feature_list = ['MAGER9', 'CIG1_R', 'CIG3_R', 'MRACEHISP', 'MEDUC', 'FAGEREC11', 'FRACEHISP', 'FEDUC', 'PRIORLIVE', 'PRIORDEAD', 'AB_NICU']

# train contains 10% of all data
reduced_feature_hyp = hypertension_df[feature_list]
train_red_feat_hyp, test_red_feat_hyp = split_sets(reduced_feature_hyp, 0, test_prop=0.9)

# import full dataset and investigate relationsship
# between DIABETES (SEE DIABETES PY FILE) and HYPERTENSION
all3wolabor = pd.read_csv('cdc3yearswolabor.csv')

# Pre_pregnancy Diabetes and Gestational Hypertension 
all3wolabor = all3wolabor[all3wolabor['AB_NICU'].isin(['Y', 'N'])]
all3wolabor = all3wolabor[all3wolabor['RF_PDIAB'].isin(['Y', 'N'])]
all3wolabor = all3wolabor[all3wolabor['RF_GHYPE'].isin(['Y', 'N'])]

print(all3wolabor.groupby('RF_PDIAB')['AB_NICU'].value_counts())

# Frequency non PDIAB
#non-NICU
print("Percent of non_RF_PDIAB NICU admissions are: %.5f" % (1007888*100/(10490493+1007888)))
#NICU
print("Percent of RF_PDIAB NICU admissions are: %.5f" % (30042*100/(74851+30042)))

all3wolabor_PDIAB = all3wolabor[all3wolabor['RF_PDIAB'] == 'Y']
all3wolabor_PDIAB_PHYPE = all3wolabor_PDIAB[all3wolabor_PDIAB['RF_PHYPE'] == 'Y']

# PREDIAB AND PREHYPE = NICU ADMISSIONS
print(all3wolabor_PDIAB_PHYPE['AB_NICU'].value_counts())
# Frequency non PDIAB
#non-NICU
print("Percent of RF_PDIAB, RF_PHYPE non-NICU admissions are: %.5f" % (10737*100/(10737+6434)))
#NICU
print("Percent of RF_PDIAB, RF_PHYPE NICU admissions are: %.5f" % (6434*100/(10737+6434)))

print(all3wolabor.groupby(['RF_PDIAB', 'RF_GHYPE'])['AB_NICU'].value_counts())
# Frequency non PDIAB
#non-NICU
print("Percent of RF_PDIAB, non-GHYPE NICU admissions are: %.5f" % (23164*100/(65083+23164)))
#NICU
print("Percent of RF_PDIAB, GHYPE NICU admissions are: %.5f" % (6878*100/(9768+6878)))

print(all3wolabor_PDIAB.groupby(['RF_PDIAB', 'RF_GHYPE'])['AB_NICU'].value_counts())
# Frequency with PDIAB non- and GHYPE
print("Percent GHYPE with PDIAB: %.5f" % ((9768+6878)*100/(65083+23164+9768+6878)))
#non-NICU
print("Percent of RF_PDIAB, non-GHYPE NICU admissions are: %.5f" % (23164*100/(65083+23164)))
#NICU
print("Percent of RF_PDIAB, GHYPE NICU admissions are: %.5f" % (6878*100/(9768+6878)))

# Higher frequency of GHYPE when PDIAB?
print(all3wolabor.groupby('RF_GHYPE').size())
print(all3wolabor.groupby(['RF_PDIAB', 'RF_GHYPE'])['RF_GHYPE'].value_counts())

# Frequency GHYPE total 
print("Percent of GHYPE in population: %.5f" % (756193*100/(10847081+756193)))
print("Percent of GHYPE among pre-pregnancy diabetics: %.5f" % (16646*100/(88247+16646)))

# Pre_pregnancy Hypertension and Gestational Diabetes
all3wolabor_hype = all3wolabor[all3wolabor['RF_PHYPE'].isin(['Y', 'N'])]
all3wolabor_hype = all3wolabor_hype[all3wolabor_hype['RF_GDIAB'].isin(['Y', 'N'])]
all3wolabor_hype = all3wolabor_hype[all3wolabor_hype['AB_NICU'].isin(['Y', 'N'])]

# Higher frequency of GHYPE when PDIAB?
print(all3wolabor_hype.groupby('RF_GDIAB').size())
print(all3wolabor_hype.groupby(['RF_PHYPE', 'RF_GDIAB'])['RF_GDIAB'].value_counts())

# Frequency GDIAB total 
print("Percent of GDIAB in population: %.5f" % (732645*100/(10870629+732645)))
print("Percent of GDIAB among pre-pregnancy hypertension: %.5f" % (32842*100/(186883+32842)))

# PREHYPE NICU ADMISSIONS
print(all3wolabor_hype.groupby('RF_PHYPE')['AB_NICU'].value_counts())
# Frequency non PHYPE
#non-NICU
print("Percent of RF_PHYPE non-NICU admissions are: %.5f" % (172591*100/(172591+47134)))
#NICU
print("Percent of RF_PHYPE NICU admissions are: %.5f" % (47134*100/(172591+47134)))

# Frequency of NICU admission of mothers with P_Hypertension
# with and without G_DIABETES
print(all3wolabor_hype.groupby(['RF_PHYPE', 'RF_GDIAB'])['AB_NICU'].value_counts())
print("Percent GDIA with PHYPE: %.5f" % ((7689+25153)*100/(7689+25153+39445+147438)))
#non-NICU
print("Percent of RF_PHYPE, non-GDIA NICU admissions are: %.5f" % (39445*100/(147438+39445)))
#NICU
print("Percent of RF_PHYPE, GDIA NICU admissions are: %.5f" % (7689*100/(25153+7689)))