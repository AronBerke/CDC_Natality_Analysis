{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pkHSfmdz52qF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = 400\n",
    "pd.options.display.max_columns = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bci28Iqi52qM"
   },
   "outputs": [],
   "source": [
    "#function to run simple SQL query from python\n",
    "def create_table_from_SQL(user, database, password, query):\n",
    "    '''\n",
    "    - A function that returns a pandas dataframe from a SQL query in python\n",
    "    ---------------\n",
    "    - user: user for your local SQL connection in string format\n",
    "    - database: schema name where your database is stored in string format\n",
    "    - password: password to access your local SQL connection in string format\n",
    "    - query: SQL query in string format; enclose with double quotes and use single quotes\n",
    "    to designate VARCHAR values within queries; use schema_name.table_name after FROM statement\n",
    "    '''\n",
    "    import mysql.connector\n",
    "    cnx = mysql.connector.connect(user=user, database=database, password=password)\n",
    "    cursor = cnx.cursor()\n",
    "    query = query\n",
    "    cursor.execute(query)\n",
    "    df = pd.DataFrame(cursor.fetchall())\n",
    "    df.columns = cursor.column_names\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hfoCi4OV52qO"
   },
   "outputs": [],
   "source": [
    "#modified version of Bettina's function which creates downsampled dataset for specific defects\n",
    "#vs overall NICU admissions\n",
    "\n",
    "def downsample_df (df, variable):\n",
    "\n",
    "    '''\n",
    "    Remove undefined information on defect presence admissions (defect == 'U'),\n",
    "    create a binary target vector, and create a \"balanced\" dataframe\n",
    "    with all defect cases and matching numbers of randomly selected non-defect cases.\n",
    "    --------------------\n",
    "    df: full dataframe\n",
    "    variable: variable or defect of interest in string format\n",
    "    '''\n",
    "\n",
    "    # remove unknown class from df\n",
    "    df_no_unknown = df[df[variable].isin(['Y', 'N'])]\n",
    "\n",
    "    # Create binary target vector, NICU = yes classified as class 0\n",
    "    df_y_n = pd.DataFrame(np.where((df_no_unknown[variable] == 'Y'), 0, 1))\n",
    "\n",
    "    # Get indicies of each class' observations\n",
    "    index_class0 = np.where(df_y_n == 0)[0]\n",
    "    index_class1 = np.where(df_y_n == 1)[0]\n",
    "\n",
    "    # Get numbers of observations in class 0\n",
    "    n_class0 = len(index_class0)\n",
    "\n",
    "    # Randomly sample the same number of observations from class 1 as in class 0, without replacement\n",
    "    np.random.seed(0)\n",
    "    index_class1_downsampled = np.random.choice(index_class1, size=n_class0, replace=False)\n",
    "\n",
    "    # Create dataframes for NICU and downsampled non-NICU\n",
    "    df_defect = df_no_unknown.iloc[index_class0]\n",
    "    df_adj_NONdefect = df_no_unknown.iloc[index_class1_downsampled]\n",
    "\n",
    "    # Append into 1 dataframe\n",
    "    df_downsampled = df_defect.append(df_adj_NONdefect)\n",
    "\n",
    "    return df_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DIxPUNJQ52qQ"
   },
   "outputs": [],
   "source": [
    "# function to split out holdout test set\n",
    "def split_sets(dataframe, seed, test_prop=0.1): \n",
    "    '''\n",
    "    - A function that splits specifically a dataframe into a train and test portion\n",
    "    - Requires multiple assignment: train, test\n",
    "    ---------------\n",
    "    - dataframe: dataframe to be split\n",
    "    - seed: set seed for reproducability\n",
    "    - test_prop: takes a float - proportion of dataframe that should be allocated to the test set\n",
    "    '''\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    testIdxes = np.random.choice(range(0,dataframe.shape[0]), size=round(dataframe.shape[0]*test_prop), replace=False)\n",
    "    trainIdxes = list(set(range(0,dataframe.shape[0])) - set(testIdxes))\n",
    "\n",
    "    train = dataframe.iloc[trainIdxes,:]\n",
    "    test  = dataframe.iloc[testIdxes,:]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SajEx89U52qS"
   },
   "outputs": [],
   "source": [
    "# begin dictionary of columns to analyze for CCHD - includes pre-pregnancy and gestational features\n",
    "# features re: delivery and labor are not useful for this use case\n",
    "variables = {'nominal_categorical':['MBSTATE_REC','MRACEHISP','MAR_P','DMAR','MEDUC','FRACEHISP',\\\n",
    "                                    'FEDUC','WIC','RF_PDIAB','RF_GDIAB','RF_PHYPE','RF_GHYPE',\\\n",
    "                                    'RF_EHYPE','RF_PPTERM','RF_INFTR','RF_FEDRG','RF_ARTEC','RF_CESAR',\\\n",
    "                                  'IP_GON','IP_SYPH','IP_CHLAM','IP_HEPB','IP_HEPC', 'PAY', 'SEX'],\\\n",
    "           'ordinal_categorical':['PRECARE', 'DOB_MM'],\\\n",
    "           'continuous':['MAGER', 'FAGECOMB','PRIORTERM','PRIORLIVE','PRIORDEAD','LBO_REC','TBO_REC',\\\n",
    "                         'ILLB_R','ILOP_R','ILP_R','PREVIS','CIG_0','CIG_1','CIG_2','CIG_3','M_Ht_In','BMI',\\\n",
    "                         'WTGAIN','RF_CESARN','OEGest_Comb'],\\\n",
    "            'target':['CA_CCHD']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPIqAF7G52qY"
   },
   "outputs": [],
   "source": [
    "#pull selected variables from 2016-2018 databases in SQL and append to a single dataframe\n",
    "query18 = \"SELECT MBSTATE_REC,MRACEHISP,MAR_P,DMAR,MEDUC,FRACEHISP,FEDUC,WIC,RF_PDIAB,RF_GDIAB,RF_PHYPE,\\\n",
    "                RF_GHYPE,RF_EHYPE,RF_PPTERM,RF_INFTR,RF_FEDRG,RF_ARTEC,RF_CESAR,IP_GON,IP_SYPH,IP_CHLAM,\\\n",
    "                IP_HEPB,IP_HEPC,PAY,SEX,PRECARE,DOB_MM,MAGER,FAGECOMB,PRIORTERM,PRIORLIVE,PRIORDEAD,\\\n",
    "                LBO_REC,TBO_REC,ILLB_R,ILOP_R,ILP_R,PREVIS,CIG_0,CIG_1,CIG_2,CIG_3,M_Ht_In,BMI,WTGAIN,\\\n",
    "                RF_CESARN,OEGest_Comb,CA_CCHD\\\n",
    "         FROM cdc.us2018\"\n",
    "\n",
    "query17 = \"SELECT MBSTATE_REC,MRACEHISP,MAR_P,DMAR,MEDUC,FRACEHISP,FEDUC,WIC,RF_PDIAB,RF_GDIAB,RF_PHYPE,\\\n",
    "                RF_GHYPE,RF_EHYPE,RF_PPTERM,RF_INFTR,RF_FEDRG,RF_ARTEC,RF_CESAR,IP_GON,IP_SYPH,IP_CHLAM,\\\n",
    "                IP_HEPB,IP_HEPC,PAY,SEX,PRECARE,DOB_MM,MAGER,FAGECOMB,PRIORTERM,PRIORLIVE,PRIORDEAD,\\\n",
    "                LBO_REC,TBO_REC,ILLB_R,ILOP_R,ILP_R,PREVIS,CIG_0,CIG_1,CIG_2,CIG_3,M_Ht_In,BMI,WTGAIN,\\\n",
    "                RF_CESARN,OEGest_Comb,CA_CCHD\\\n",
    "         FROM cdc.us2017\"\n",
    "\n",
    "query16 = \"SELECT MBSTATE_REC,MRACEHISP,MAR_P,DMAR,MEDUC,FRACEHISP,FEDUC,WIC,RF_PDIAB,RF_GDIAB,RF_PHYPE,\\\n",
    "                RF_GHYPE,RF_EHYPE,RF_PPTERM,RF_INFTR,RF_FEDRG,RF_ARTEC,RF_CESAR,IP_GON,IP_SYPH,IP_CHLAM,\\\n",
    "                IP_HEPB,IP_HEPC,PAY,SEX,PRECARE,DOB_MM,MAGER,FAGECOMB,PRIORTERM,PRIORLIVE,PRIORDEAD,\\\n",
    "                LBO_REC,TBO_REC,ILLB_R,ILOP_R,ILP_R,PREVIS,CIG_0,CIG_1,CIG_2,CIG_3,M_Ht_In,BMI,WTGAIN,\\\n",
    "                RF_CESARN,OEGest_Comb,CA_CCHD\\\n",
    "         FROM cdc.us2016\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4G9fmWl52qc"
   },
   "outputs": [],
   "source": [
    "def mlp_convert_cont_floats(df):\n",
    "    #convert continuous variables to float\n",
    "    #update to split out ordinal categorical separately\n",
    "    for x in variables['continuous']:\n",
    "        df[x]=df[x].astype('float')\n",
    "    return df\n",
    "\n",
    "def mlp_convert_nom_cat(df):\n",
    "    #convert nominal categorical variables to category\n",
    "    for x in variables['nominal_categorical']:\n",
    "        df[x]=df[x].astype('category')\n",
    "    return df\n",
    "\n",
    "def mlp_convert_ord_cat(df):\n",
    "    #convert ordinal categorical variables\n",
    "    df.DOB_MM = df.DOB_MM.astype('category')\n",
    "    df.PRECARE = df.PRECARE.astype('float')\n",
    "    return df\n",
    "\n",
    "def mlp_fill_MAR_blanks(df):\n",
    "    #change true nulls to fit missingness definitions already in the dataset\n",
    "    df.isnull().sum()\n",
    "    df.MAR_P = df.MAR_P.fillna(value='U')\n",
    "    df.MAR_P.replace('','U',inplace=True)\n",
    "    df.DMAR = df.DMAR.fillna(value=9)\n",
    "    df.DMAR.replace('',9, inplace=True) # need to take care of 1 vs '1'\n",
    "    df.DMAR.replace('1',1, inplace=True)\n",
    "    df.DMAR.replace('2',2, inplace=True)\n",
    "    return df\n",
    "\n",
    "def mlp_reassign_FRACE(df):\n",
    "    #combine FRACEHISP unknowns columns\n",
    "    df.FRACEHISP = df.FRACEHISP.replace(8,9)\n",
    "    return df\n",
    "\n",
    "def mlp_reassign_X_NA(df):\n",
    "    #assign 'X' to 'N' for RF_FEDRG RF_ARTEC and 'Y' for MAR_P since paternity assumed for married\n",
    "    for x in ['RF_FEDRG', 'RF_ARTEC']:\n",
    "        df[x].replace('X','N', inplace=True)\n",
    "    df.MAR_P.replace('X','Y', inplace=True)\n",
    "    return df\n",
    "\n",
    "def mlp_reassign_ILs(df):\n",
    "    #assign 888 to mean for ILLB_R and ILP_R\n",
    "    for x in ['ILLB_R', 'ILP_R', 'ILOP_R']:\n",
    "        print(x)\n",
    "        df[x].replace(888,df.MAGER*12, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYpejP5y52qe"
   },
   "outputs": [],
   "source": [
    "#create missingness types:\n",
    "missing_dict = {'cont9': ['LBO_REC', 'TBO_REC'],\\\n",
    "                'cont99': ['FAGECOMB', 'PRIORTERM','PRIORLIVE', 'PRIORDEAD', 'PRECARE', 'PREVIS',\\\n",
    "                         'CIG_0', 'CIG_1', 'CIG_2', 'CIG_3', 'M_Ht_In', 'WTGAIN', 'RF_CESARN', 'OEGest_Comb'],\\\n",
    "                'cont999':['ILLB_R', 'ILP_R', 'ILOP_R'],\\\n",
    "                'cont99.9': ['BMI'],\\\n",
    "                'cat3': ['MBSTATE_REC'],\\\n",
    "                'cat8': ['MRACEHISP'],\\\n",
    "                'cat9': ['MEDUC', 'FEDUC', 'PAY', 'FRACEHISP', 'DMAR'],\\\n",
    "                'catU': ['WIC','RF_PDIAB','RF_GDIAB','RF_PHYPE',\\\n",
    "                        'RF_GHYPE','RF_EHYPE','RF_PPTERM','RF_INFTR','RF_FEDRG','RF_ARTEC','RF_CESAR','IP_GON',\\\n",
    "                        'IP_SYPH','IP_CHLAM','IP_HEPB','IP_HEPC', 'MAR_P']}\n",
    "missing_vals = [9,99,999,99.9,3,8,9,'U']\n",
    "\n",
    "def measure_missing(df, target):\n",
    "    #create table of missingness proportions\n",
    "    missing_props = pd.DataFrame()\n",
    "    for i in range(0,len(missing_vals)):\n",
    "        temp = df.groupby(target)[missing_dict[list(missing_dict.keys())[i]]].apply\\\n",
    "        (lambda x: np.sum(x==missing_vals[i])/(df.shape[0]/2))\n",
    "        missing_props = pd.concat([missing_props, temp], axis=1)   \n",
    "\n",
    "    #create lists of variables with high missingness vs. low missingness\n",
    "    large_miss = list(missing_props.columns[missing_props.apply(lambda x: sum(x)>0.1, axis=0)])\n",
    "    small_miss = list(missing_props.columns[missing_props.apply(lambda x: sum(x)<0.1, axis=0)])\n",
    "\n",
    "    #sort low missingness categorical variables into types\n",
    "    small_cats = {'cat3': [], 'cat8': [], 'cat9': [], 'catU': []}\n",
    "\n",
    "    for var in small_miss:\n",
    "        if var in missing_dict['cat3']:\n",
    "            small_cats['cat3'].append(var) \n",
    "        elif var in missing_dict['cat8']:\n",
    "            small_cats['cat8'].append(var)\n",
    "        elif var in missing_dict['cat9']:\n",
    "            small_cats['cat9'].append(var)\n",
    "        elif var in missing_dict['catU']:\n",
    "            small_cats['catU'].append(var)    \n",
    "\n",
    "\n",
    "    #sort low missingness continuous variables\n",
    "    small_conts = {'cont9': [], 'cont99': [], 'cont999': [], 'cont99.9': []}\n",
    "\n",
    "    for var in small_miss:\n",
    "        if var in missing_dict['cont9']:\n",
    "            small_conts['cont9'].append(var) \n",
    "        elif var in missing_dict['cont99']:\n",
    "            small_conts['cont99'].append(var)\n",
    "        elif var in missing_dict['cont999']:\n",
    "            small_conts['cont999'].append(var)\n",
    "        elif var in missing_dict['cont99.9']:\n",
    "            small_conts['cont99.9'].append(var)\n",
    "    return small_conts, small_cats, large_miss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l94IWnxV52qh"
   },
   "outputs": [],
   "source": [
    "def mlp_impute_s_cat(df,df_w,small_cats):\n",
    "    #mode imputation of categoricals with low missingness\n",
    "    small_vals = [3,8,9,'U']\n",
    "    for i in range(0, len(small_vals)):\n",
    "        temp_lis = small_cats[list(small_cats.keys())[i]]\n",
    "        for x in temp_lis:\n",
    "            major_cat = df_w[x].value_counts().sort_values(ascending=False).index[0]\n",
    "            df[x]=df[x].replace(small_vals[i],major_cat)\n",
    "    return df\n",
    "\n",
    "def mlp_impute_s_num(df,df_w,small_conts):\n",
    "    #median imputation of categoricals with low missingness\n",
    "    #statistical significance of relationship with target imnproves on variable by variable basis after \n",
    "    #median imputation\n",
    "    csmall_vals = [9,99,999,99.9]\n",
    "    for i in range(0, len(csmall_vals)):\n",
    "        temp_lis = small_conts[list(small_conts.keys())[i]]\n",
    "        for x in temp_lis:\n",
    "            df[x]=df[x].replace(csmall_vals[i],df_w[x].median())\n",
    "    return df\n",
    "\n",
    "\n",
    "def binarize9(x):\n",
    "    if x==9:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def binarize99(x):\n",
    "    if x==99:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def binarize999(x):\n",
    "    if x==999:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def mlp_impute_FAGECOMB(df,df_w):\n",
    "    #Impute FAGECOMB missing vals and store whether column was imputed    \n",
    "    df['FAGECOMB_IMP'] = df.FAGECOMB.apply(lambda x: binarize99(x))\n",
    "    df.FAGECOMB.replace(99, df_w.FAGECOMB.median(),inplace = True)\n",
    "    return df\n",
    "\n",
    "def mlp_impute_ILPs(df,df_w):\n",
    "    #Impute ILOP_R and ILP_R missing vals and store whether column was imputed\n",
    "    for x in ['ILOP_R', 'ILP_R']:\n",
    "        df[x+'_IMP'] = df[x].apply(lambda x: binarize999(x))\n",
    "    for x in ['ILOP_R', 'ILP_R']:\n",
    "        df[x].replace(999,df_w[x].median(), inplace=True)\n",
    "    return df\n",
    "\n",
    "def mlp_impute_FRACE_ED(df,df_w):\n",
    "    #Impute FRACEHISP and FEDUC missing vals and store whether column was imputed\n",
    "    for x in ['FRACEHISP', 'FEDUC']:\n",
    "        df[x+'_IMP'] = df[x].apply(lambda x: binarize9(x))\n",
    "    for x in ['FRACEHISP', 'FEDUC']:\n",
    "        df[x].replace(9,df_w[x].mode()[0], inplace=True)\n",
    "    return df\n",
    "\n",
    "def mlp_impute_combine(df):\n",
    "    #Combine imputed flag columns into one\n",
    "    import re\n",
    "    imputed_col = list(filter(lambda i: re.search('_IMP',i), df.columns))\n",
    "    print(len(imputed_col))\n",
    "    if len(imputed_col)==0:\n",
    "        return df\n",
    "    df['lrg_miss_imp']= [1 if sum(df[imputed_col].iloc[i])==1 else 0 for i in range(len(df))]\n",
    "    df.drop(columns = imputed_col, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rmMbBKU152qj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SfwgWZ9J52qm"
   },
   "outputs": [],
   "source": [
    "#dummfiy columns\n",
    "\n",
    "def dummify_columns(dataframe,var_list):\n",
    "    '''\n",
    "    dummifies a columns, merges with the dataframe, and drops the non-dummified column\n",
    "    ------------\n",
    "    dataframe: full dataframe\n",
    "    variable: column name as string\n",
    "    '''\n",
    "    for vr in var_list:\n",
    "        dummified_feature = pd.get_dummies(dataframe[vr], prefix=vr,drop_first=True,prefix_sep='__')\n",
    "        dataframe = pd.concat([dataframe,dummified_feature],axis=1,sort=False)\n",
    "    dataframe.drop(columns = var_list, inplace = True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cf5F6Sqt52qq"
   },
   "outputs": [],
   "source": [
    "#standardize columns\n",
    "\n",
    "def standardize_columns(df,var_list):\n",
    "    '''\n",
    "    standardize a columns, merges with the dataframe, and drops the non-standardized column\n",
    "    ------------\n",
    "    dataframe: full dataframe\n",
    "    variable: column name as string\n",
    "    '''\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaleit = StandardScaler()\n",
    "    for vr in var_list:\n",
    "\n",
    "        scaled_feature = pd.DataFrame(scaleit.fit_transform(df[[vr]]), index = df.index, columns=[vr+'__S'])\n",
    "        \n",
    "        df = pd.concat([df,scaled_feature],axis=1,sort=False)\n",
    "    df.drop(columns = var_list, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HDRHYBp52qt"
   },
   "outputs": [],
   "source": [
    "def add_random_column_to_df (dataframe):\n",
    "    import random\n",
    "    mylist = []\n",
    "    for i in range(0,dataframe.shape[0]):\n",
    "        x = random.randint(1,1000)\n",
    "        mylist.append(x)\n",
    "    dataframe['RANDOM'] = mylist\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mt7RRw8652qv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72ryN7XW52qx"
   },
   "outputs": [],
   "source": [
    "def mlp_all_of_the_above(df,df_w,target):\n",
    "    df = mlp_fill_MAR_blanks(df)\n",
    "    df = mlp_reassign_FRACE(df)\n",
    "    df = mlp_reassign_X_NA(df)\n",
    "#     df = mlp_reassign_ILs(df)\n",
    "    df = mlp_convert_cont_floats(df)\n",
    "    small_conts, small_cats, large_miss = measure_missing(df,target)\n",
    "    df = mlp_impute_s_cat(df,df_w,small_cats)\n",
    "    df = mlp_impute_s_num(df,df_w,small_conts)\n",
    "    df = mlp_impute_FAGECOMB(df,df_w)\n",
    "    df = mlp_impute_ILPs(df,df_w)\n",
    "    df = mlp_impute_FRACE_ED(df,df_w)\n",
    "    df = mlp_convert_nom_cat(df)\n",
    "    df = mlp_convert_ord_cat(df)\n",
    "#     df = mlp_impute_combine(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "jnKepKFU52qa",
    "outputId": "80afd562-e320-4681-eacf-1eabad04ec85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nqueries = [query18, query17, query16]\\n            \\ncchd = pd.DataFrame()\\ntest_cchd = pd.DataFrame()\\n\\nfor query in queries:\\n    temp = create_table_from_SQL('root','cdc',sql_pw, query)\\n    train, test = split_sets(temp, 0, test_prop=0.1)\\n    train = downsample_df(train, 'CA_CCHD')\\n    cchd = cchd.append(train)  \\n    test_cchd = test_cchd.append(test)\\n\""
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [query18, query17, query16]\n",
    "            \n",
    "cchd = pd.DataFrame()\n",
    "test_cchd = pd.DataFrame()\n",
    "\n",
    "for query in queries:\n",
    "    temp = create_table_from_SQL('root','cdc',sql_pw, query)\n",
    "    train, test = split_sets(temp, 0, test_prop=0.1)\n",
    "    train = downsample_df(train, 'CA_CCHD')\n",
    "    cchd = cchd.append(train)  \n",
    "    test_cchd = test_cchd.append(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z53k2ece52q2"
   },
   "outputs": [],
   "source": [
    "chd=cchd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUlmv2hd52q6"
   },
   "outputs": [],
   "source": [
    "chd_test = test_cchd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-H8uQ9Y8AH_"
   },
   "outputs": [],
   "source": [
    "target = 'CA_CCHD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kq50yjg552q9"
   },
   "outputs": [],
   "source": [
    "chd = mlp_all_of_the_above(chd,chd,'CA_CCHD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCvd_XK852rA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chd_test = chd_test.loc[(chd_test[target]=='Y')|(chd_test[target]=='N'),:]\n",
    "chd_test = mlp_all_of_the_above(chd_test,chd,'CA_CCHD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bnawptL052rC"
   },
   "outputs": [],
   "source": [
    "chd_test.MAR_P = chd_test.MAR_P.replace('U','Y')\n",
    "chd_test.MAR_P = chd_test.MAR_P.astype('str').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hL398O2l52rO"
   },
   "outputs": [],
   "source": [
    "#PHYPE\n",
    "chd = chd[chd['RF_PHYPE'] == 'Y']\n",
    "chd_test = chd_test[chd_test['RF_PHYPE'] == 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Vh-gkcZ52rE"
   },
   "outputs": [],
   "source": [
    "chd['lrg_miss_imp'] = chd.FAGECOMB_IMP | chd.FRACEHISP_IMP | chd.ILOP_R_IMP | chd.ILP_R_IMP | chd.FEDUC_IMP\n",
    "chd.drop(columns = ['FAGECOMB_IMP','FRACEHISP_IMP','ILOP_R_IMP','ILP_R_IMP','FEDUC_IMP'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hk3AHigR52rG"
   },
   "outputs": [],
   "source": [
    "chd = standardize_columns(chd,list(set(chd.select_dtypes(exclude = ['category','object']).columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNA-9EjD52rL"
   },
   "outputs": [],
   "source": [
    "target = 'CA_CCHD'\n",
    "dummified_r = dummify_columns(chd,list(set(chd.select_dtypes(include ='category').columns)))\n",
    "y_train = dummify_columns(dummified_r[[target]],[target])\n",
    "X_train = dummified_r.loc[:,set(dummified_r.columns) - set([target])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnPtXEoD9kNJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TBLZKU_b52rR"
   },
   "outputs": [],
   "source": [
    "chd_test['lrg_miss_imp'] = chd_test.FAGECOMB_IMP | chd_test.FRACEHISP_IMP | chd_test.ILOP_R_IMP | chd_test.ILP_R_IMP | chd_test.FEDUC_IMP\n",
    "chd_test.drop(columns = ['FAGECOMB_IMP','FRACEHISP_IMP','ILOP_R_IMP','ILP_R_IMP','FEDUC_IMP'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7-C2OIZ52rU"
   },
   "outputs": [],
   "source": [
    "chd_test = standardize_columns(chd_test,list(chd_test.select_dtypes(exclude = ['category','object']).columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h82iYw_G52rY"
   },
   "outputs": [],
   "source": [
    "target = 'CA_CCHD'\n",
    "dummified_e = dummify_columns(chd_test,list(set(chd_test.select_dtypes(include ='category').columns)))\n",
    "y_test = dummify_columns(dummified_e[[target]],[target])\n",
    "X_test = dummified_e.loc[:,set(dummified_e.columns) - set([target])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wb500dSm52re"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "nzH6n3zI52rg",
    "outputId": "d3e6ab79-b518-4a43-f01a-e975f6fb61b3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 169 ms, sys: 74.8 ms, total: 244 ms\n",
      "Wall time: 3.13 s\n",
      "{'n_neighbors': 100}\n",
      "0.6049595443534838\n",
      "CPU times: user 1.77 s, sys: 12.2 ms, total: 1.79 s\n",
      "Wall time: 1.79 s\n",
      "0.07756614668620185\n",
      "[[10523 11594]\n",
      " [   25     9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "y_train_fit = pd.DataFrame([-1 if i == 0 else 1 for i in y_train[list(y_train.columns)[0]]])\n",
    "\n",
    "lofcv = LocalOutlierFactor(contamination = 0.5, novelty = True)\n",
    "lof_grid_param = {'n_neighbors': range(20,101,10),}\n",
    "gsearch = GridSearchCV(lofcv,lof_grid_param,scoring='precision',n_jobs=2,cv=5)\n",
    "%time gsearch.fit(X_train, y_train_fit)\n",
    "\n",
    "print(gsearch.best_params_)\n",
    "print(gsearch.best_score_)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_test_fit = pd.DataFrame([-1 if i == 0 else 1 for i in y_test[list(y_test.columns)[0]]])\n",
    "%time cm = confusion_matrix(y_test_fit,gsearch.predict(X_test))\n",
    "print(cm[1,1]/sum(cm[:,1])*100)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCr-MOev52ri"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLC6zAWH52rk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSF7K76V52rt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PHYPE_LOF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
